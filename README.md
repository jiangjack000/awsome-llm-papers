# awsome-llm-papers
A comprehensive repository for research papers, code snippets, and notes related to llms.

## BERT
BERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based machine learning technique for natural language processing (NLP) pre-training. It was created and published in 2018 by Jacob Devlin and his colleagues from Google.

Paper: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

## GPT-1
GPT-1 (Generative Pretrained Transformer 1) is a language generation model that uses transformer model's architecture for training. It was released by OpenAI in June 2018.

Paper: [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

## GPT-2
GPT-2 is the second generation Generative Pretrained Transformer model released by OpenAI. It has been trained with a diverse range of internet text, but without any specific task in mind.

Paper: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

## GPT-3
GPT-3, the third iteration of the GPT models, is considered one of the most powerful and largest language models to date, with 175 billion machine learning parameters.

Paper: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

## DeepSeek-V3

 DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2.

Paper: [DeepSeek-V3](./papers/DeepSeek_V3.pdf)


## DeepSeek-R1

 DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. 
Paper: [DeepSeek-R1](./papers/DeepSeek_R1.pdf)


## LLama3.1
The Llama 3 Herd
of models natively supports multilinguality, coding, reasoning, and tool usage. Our largest model is dense
Transformer with 405B parameters, processing information in a context window of up to 128K tokens.

Paper: [LLama3.1](./papers/llama3.1.pdf)

