# awsome-llm-papers
A comprehensive repository for research papers, code snippets, and notes related to llms.

## BERT
BERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based machine learning technique for natural language processing (NLP) pre-training. It was created and published in 2018 by Jacob Devlin and his colleagues from Google.

Paper: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

## GPT-1
GPT-1 (Generative Pretrained Transformer 1) is a language generation model that uses transformer model's architecture for training. It was released by OpenAI in June 2018.

Paper: [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

## GPT-2
GPT-2 is the second generation Generative Pretrained Transformer model released by OpenAI. It has been trained with a diverse range of internet text, but without any specific task in mind.

Paper: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

## GPT-3
GPT-3, the third iteration of the GPT models, is considered one of the most powerful and largest language models to date, with 175 billion machine learning parameters.

Paper: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)