# awsome-llm-papers
A comprehensive repository for research papers, code snippets, and notes related to llms.
## Attention Is All You Need

The "Attention Is All You Need" paper introduces the Transformer architecture, which relies entirely on self-attention mechanisms, dispensing with recurrence and convolution entirely. This architecture has become the foundation for many state-of-the-art models in natural language processing.

Paper: [Attention Is All You Need](./papers/Attention_Is_All_You_Need.pdf)

---

## BERT

BERT (Bidirectional Encoder Representations from Transformers) presents a novel approach to pre-training language representations. By leveraging transformers and training bidirectionally, BERT achieves superior performance on a wide range of natural language understanding tasks.

Paper: [BERT](./papers/BERT.pdf)

---

## DPO

DPO (Direct Preference Optimization) focuses on optimizing models based on direct human preferences. This approach enhances model alignment with user intentions by incorporating preference data directly into the training process.

Paper: [DPO](./papers/DPO.pdf)

---

## DeepSeek_R1

DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.

Paper: [DeepSeek-R1](./papers/DeepSeek_R1.pdf)

---

## DeepSeek_V3

DeepSeek_V3 introduces enhancements over its predecessors by incorporating advanced optimization techniques and expanded training data, resulting in improved accuracy and efficiency in complex reasoning tasks.

Paper: [DeepSeek_V3](./papers/DeepSeek_V3.pdf)

---

## ERNIE - Enhanced Language Representation with Informative Entities

ERNIE enhances language representation by integrating structured knowledge about entities. By embedding informative entities into the model, ERNIE achieves better performance on tasks requiring deep understanding of entity relationships and semantics.

Paper: [ERNIE - Enhanced Language Representation with Informative Entities](./papers/ERNIE_Enhanced_Language_Representation_with_Informative_Entities.pdf)

---

## Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer

This paper explores the capabilities of a unified text-to-text framework for transfer learning. By converting all text-based language problems into a text-to-text format, the model achieves strong performance across diverse NLP tasks.

Paper: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](./papers/Exploring_the_Limits_of_Transfer_Learning_with_a_Unified_Text-to-Text_Transformer.pdf)

---

## Improving Language Understanding by Generative Pre-Training

The Generative Pre-Training (GPT) model improves language understanding by pre-training on a large corpus of text data and then fine-tuning on specific tasks. This approach leverages unsupervised learning to enhance performance on various NLP applications.

Paper: [Improving Language Understanding by Generative Pre-Training](./papers/Improving_Language_Understanding_by_Generative_Pre-Training.pdf)

---

## Language Models are Few-Shot Learners

This paper demonstrates that large language models, like GPT-3, can perform tasks with little to no task-specific training. By leveraging few-shot learning, these models show impressive versatility and understanding across a wide array of applications.

Paper: [Language Models are Few-Shot Learners](./papers/Language_Models_are_Few-Shot_Learners.pdf)

---

## Learning Transferable Visual Models From Natural Language Supervision

The CLIP model learns visual representations by training on image-text pairs from the internet. This approach enables the model to generalize across various visual concepts and perform zero-shot classification based on natural language descriptions.

Paper: [Learning Transferable Visual Models From Natural Language Supervision](./papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf)

---

## NeurIPS-2022-training-language-models-to-follow-instructions-with-human-feedback-Paper-Conference

This paper presents methods for training language models to follow human instructions more effectively using reinforcement learning from human feedback. The approach enhances model alignment with user intentions and improves response quality.

Paper: [NeurIPS-2022 Training Language Models to Follow Instructions with Human Feedback](./papers/NeurIPS-2022-training-language-models-to-follow-instructions-with-human-feedback-Paper-Conference.pdf)

---

## Qwen2_5_1M_Technical_Report

Qwen2 introduces a highly efficient language model with 5.1 million parameters. This technical report details the model architecture, training procedures, and performance evaluations, demonstrating its applicability in various NLP tasks.

Paper: [Qwen2_5_1M_Technical_Report](./papers/Qwen2_5_1M_Technical_Report.pdf)

---

## RoBERTa - A Robustly Optimized BERT Pretraining Approach

RoBERTa optimizes the BERT pretraining process by removing the next sentence prediction objective and training with larger mini-batches and learning rates. These adjustments lead to significant performance improvements across multiple NLP benchmarks.

Paper: [RoBERTa - A Robustly Optimized BERT Pretraining Approach](./papers/RoBERTa_A_Robustly_Optimized_BERT_Pretraining_Approach.pdf)

---

## Swin Transformer - Hierarchical Vision Transformer using Shifted Windows

The Swin Transformer introduces a hierarchical structure using shifted windows for attention computation. This architecture enables efficient scaling to high-resolution images and improves performance on various computer vision tasks.

Paper: [Swin Transformer - Hierarchical Vision Transformer using Shifted Windows](./papers/Swin_Transformer_Hierarchical_Vision_Transformer_using_Shifted_Windows.pdf)

---

## Visual Instruction Tuning

Visual Instruction Tuning focuses on aligning multimodal models with human instructions using visual data. This approach enhances the model's ability to understand and execute tasks that involve both textual and visual inputs.

Paper: [Visual Instruction Tuning](./papers/Visual_Instruction_Tuning.pdf)

---

## XLNet - Generalized Autoregressive Pretraining for Language Understanding

XLNet combines autoregressive modeling with autoencoding to capture bidirectional context without the limitations of BERT. This generalized pretraining approach leads to improved performance on a wide range of language understanding tasks.

Paper: [XLNet - Generalized Autoregressive Pretraining for Language Understanding](./papers/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding.pdf)

---

## LLama3.1

The Llama 3 Herd of models natively supports multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters, processing information in a context window of up to 128K tokens.

Paper: [LLama3.1](./papers/llama3.1.pdf)

---

## pre train 2 OLMO2

Pre Train 2 OLMO2 discusses the advancements in pretraining strategies for the OLMO2 model. It highlights improvements in training efficiency, model scalability, and performance across various natural language processing tasks.

Paper: [pre_train_2_OLMO2](./papers/pre_train_2_OLMO2.pdf)
